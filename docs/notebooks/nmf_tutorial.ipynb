{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Online Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks explains basic ideas behind NMF implementation, training examples and use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Matrix Factorizations** are useful for many things: recomendation systems, bi-clustering, image compression and, in particular, topic modeling.\n",
    "\n",
    "Why **Non-Negative**? It makes the problem more strict and allows us to apply some optimizations.\n",
    "\n",
    "Why **Online**? Because corpora are large and RAM is limited. Online NMF can learn topics iteratively.\n",
    "\n",
    "This particular implementation is based on [this paper](https://arxiv.org/abs/1604.02634).\n",
    "\n",
    "The main attributes are following:\n",
    "\n",
    "- W is a word-topic matrix\n",
    "- h is a topic-document matrix\n",
    "- v is an input word-document matrix\n",
    "\n",
    "The idea of the algorithm is as follows:\n",
    "\n",
    "```\n",
    "Initialize W, A and B matrices\n",
    "\n",
    "Input corpus\n",
    "Split corpus to batches\n",
    "\n",
    "for v in batches:\n",
    "    infer h:\n",
    "        do coordinate gradient descent step to find h that minimizes (v - Wh) l2 norm\n",
    "        bound h so that it is non-negative\n",
    "\n",
    "    update A and B\n",
    "\n",
    "    update W:\n",
    "        do gradient descent for W using A and B values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in this tutorial?\n",
    "\n",
    "- Basic training example\n",
    "- Comparison with alternative models (LDA and Sklearn NMF)\n",
    "- Non-standart application (image decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anotherbugmaster/.virtualenvs/gensim/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from numpy.random import RandomState\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition.nmf import NMF as SklearnNmf\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim import matutils\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim.models.nmf import Nmf as GensimNmf\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = api.load('20-newsgroups')\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'rec.motorcycles',\n",
    "    'talk.politics.mideast',\n",
    "    'sci.space'\n",
    "]\n",
    "\n",
    "categories = {\n",
    "    name: idx\n",
    "    for idx, name\n",
    "    in enumerate(categories)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = RandomState(42)\n",
    "\n",
    "trainset = np.array([\n",
    "    {\n",
    "        'data': doc['data'],\n",
    "        'target': categories[doc['topic']],\n",
    "    }\n",
    "    for doc\n",
    "    in newsgroups\n",
    "    if doc['topic'] in categories\n",
    "    and doc['set'] == 'train'\n",
    "])\n",
    "random_state.shuffle(trainset)\n",
    "\n",
    "testset = np.array([\n",
    "    {\n",
    "        'data': doc['data'],\n",
    "        'target': categories[doc['topic']],\n",
    "    }\n",
    "    for doc\n",
    "    in newsgroups\n",
    "    if doc['topic'] in categories\n",
    "    and doc['set'] == 'test'\n",
    "])\n",
    "random_state.shuffle(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = [preprocess_string(doc['data']) for doc in trainset]\n",
    "test_documents = [preprocess_string(doc['data']) for doc in testset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-30 16:47:24,148 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-01-30 16:47:24,744 : INFO : built Dictionary(25279 unique tokens: ['actual', 'assum', 'babbl', 'batka', 'batkaj']...) from 2819 documents (total 435328 corpus positions)\n",
      "2019-01-30 16:47:24,794 : INFO : discarding 18198 tokens: [('batka', 1), ('batkaj', 1), ('beatl', 1), ('ccmail', 3), ('dayton', 4), ('edu', 1785), ('inhibit', 1), ('jbatka', 1), ('line', 2748), ('organ', 2602)]...\n",
      "2019-01-30 16:47:24,795 : INFO : keeping 7081 tokens which were in no less than 5 and no more than 1409 (=50.0%) documents\n",
      "2019-01-30 16:47:24,813 : INFO : resulting dictionary: Dictionary(7081 unique tokens: ['actual', 'assum', 'babbl', 'burster', 'caus']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(train_documents)\n",
    "dictionary.filter_extremes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [\n",
    "    dictionary.doc2bow(document)\n",
    "    for document\n",
    "    in train_documents\n",
    "]\n",
    "\n",
    "test_corpus = [\n",
    "    dictionary.doc2bow(document)\n",
    "    for document\n",
    "    in test_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The API works in the way similar to [Gensim.models.LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html).\n",
    "\n",
    "Special parameters:\n",
    "\n",
    "- `kappa` float, optional\n",
    "\n",
    "    Gradient descent step size.\n",
    "    \n",
    "    Larger value makes the model train faster, but could lead to non-convergence if set too large.\n",
    "    \n",
    "    \n",
    "- `w_max_iter` int, optional\n",
    "\n",
    "    Maximum number of iterations to train W per each batch.\n",
    "    \n",
    "    \n",
    "- `w_stop_condition` float, optional\n",
    "\n",
    "    If error difference gets less than that, training of ``W`` stops for the current batch.\n",
    "    \n",
    "    \n",
    "- `h_r_max_iter` int, optional\n",
    "\n",
    "    Maximum number of iterations to train h per each batch.\n",
    "    \n",
    "    \n",
    "- `h_r_stop_condition` float\n",
    "\n",
    "    If error difference gets less than that, training of ``h`` stops for the current batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-30 16:47:26,180 : INFO : Loss: 1.0280021673693736\n",
      "2019-01-30 16:47:26,374 : INFO : Loss: 0.9805869534381415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 697 ms, sys: 304 Âµs, total: 698 ms\n",
      "Wall time: 696 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = GensimNmf(\n",
    "    corpus=train_corpus,\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    chunksize=1000,\n",
    "    passes=5,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0,\n",
    "    random_state=42,\n",
    "    kappa=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"armenian\" + 0.015*\"peopl\" + 0.014*\"said\" + 0.013*\"know\" + 0.008*\"went\" + 0.008*\"sai\" + 0.007*\"like\" + 0.007*\"apart\" + 0.007*\"come\" + 0.007*\"azerbaijani\"'),\n",
       " (1,\n",
       "  '0.074*\"jpeg\" + 0.032*\"file\" + 0.031*\"gif\" + 0.028*\"imag\" + 0.024*\"color\" + 0.017*\"format\" + 0.014*\"qualiti\" + 0.013*\"convert\" + 0.013*\"compress\" + 0.013*\"version\"'),\n",
       " (2,\n",
       "  '0.030*\"imag\" + 0.014*\"graphic\" + 0.012*\"data\" + 0.010*\"file\" + 0.010*\"pub\" + 0.010*\"ftp\" + 0.010*\"avail\" + 0.008*\"format\" + 0.008*\"program\" + 0.008*\"packag\"'),\n",
       " (3,\n",
       "  '0.015*\"god\" + 0.012*\"atheist\" + 0.009*\"believ\" + 0.009*\"exist\" + 0.008*\"atheism\" + 0.007*\"peopl\" + 0.007*\"religion\" + 0.006*\"christian\" + 0.006*\"israel\" + 0.006*\"religi\"'),\n",
       " (4,\n",
       "  '0.028*\"space\" + 0.019*\"launch\" + 0.013*\"satellit\" + 0.009*\"orbit\" + 0.008*\"nasa\" + 0.007*\"year\" + 0.007*\"mission\" + 0.006*\"new\" + 0.006*\"commerci\" + 0.005*\"market\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-30 16:47:26,455 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.7053902612634844"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoherenceModel(\n",
    "    model=nmf,\n",
    "    corpus=test_corpus,\n",
    "    coherence='u_mass'\n",
    ").get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2501.280703411481"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-nmf.log_perplexity(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Document topics inference\n",
    "\n",
    "Let's get some news and infer a topic vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: spl@ivem.ucsd.edu (Steve Lamont)\n",
      "Subject: Re: RGB to HVS, and back\n",
      "Organization: University of Calif., San Diego/Microscopy and Imaging Resource\n",
      "Lines: 18\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: ivem.ucsd.edu\n",
      "\n",
      "In article <ltu4buINNe7j@caspian.usc.edu> zyeh@caspian.usc.edu (zhenghao yeh) writes:\n",
      ">|> See Foley, van Dam, Feiner, and Hughes, _Computer Graphics: Principles\n",
      ">|> and Practice, Second Edition_.\n",
      ">|> \n",
      ">|> [If people would *read* this book, 75 percent of the questions in this\n",
      ">|> froup would disappear overnight...]\n",
      ">|> \n",
      ">\tNot really. I think it is less than 10%.\n",
      "\n",
      "Nah... I figure most people would be so busy reading that they wouldn't\n",
      "have *time* to post. :-) :-) :-)\n",
      "\n",
      "\t\t\t\t\t\t\tspl\n",
      "-- \n",
      "Steve Lamont, SciViGuy -- (619) 534-7968 -- spl@szechuan.ucsd.edu\n",
      "San Diego Microscopy and Imaging Resource/UC San Diego/La Jolla, CA 92093-0608\n",
      "\"Until I meet you, then, in Upper Hell\n",
      "Convulsed, foaming immortal blood: farewell\" - J. Berryman, \"A Professor's Song\"\n",
      "\n",
      "====================================================================================================\n",
      "Topics: [(0, 0.29903293372372697), (1, 0.007751538808305081), (2, 0.41698421255575224), (3, 0.27623131491221575)]\n"
     ]
    }
   ],
   "source": [
    "print(testset[0]['data'])\n",
    "print('=' * 100)\n",
    "print(\"Topics: {}\".format(nmf[test_corpus[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word topic inference\n",
    "\n",
    "Here's an example of topic distribution inference for a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: actual\n",
      "Topics: [(0, 0.04910674896578284), (1, 0.1277766177062051), (2, 0.07803764680331245), (3, 0.6584104509982174), (4, 0.08666853552648228)]\n"
     ]
    }
   ],
   "source": [
    "word = dictionary[0]\n",
    "print(\"Word: {}\".format(word))\n",
    "print(\"Topics: {}\".format(nmf.get_term_topics(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def density(matrix):\n",
    "    return (matrix > 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term-topic matrix of shape `(words, topics)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density: 0.6427905663041943\n"
     ]
    }
   ],
   "source": [
    "print(\"Density: {}\".format(density(nmf._W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic-document matrix for the last batch of shape `(topics, batch)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density: 0.8424908424908425\n"
     ]
    }
   ],
   "source": [
    "print(\"Density: {}\".format(density(nmf._h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals matrix of the last batch of shape `(words, batch)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim NMF vs Sklearn NMF vs Gensim LDA\n",
    "\n",
    "We'll run all the models on the [20newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) dataset, which has texts and labels for them.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- train time: time to train a model in seconds\n",
    "- coherence: coherence score (not defined for sklearn NMF). Classic metric for topic models.\n",
    "- perplexity: perplexity score. Another usual TM metric\n",
    "- f1: f1 on the task of news topic classification\n",
    "- l2_norm: l2 matrix norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    corpus=train_corpus,\n",
    "    chunksize=1000,\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    passes=5,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_execution_time(func):\n",
    "    start = time.time()\n",
    "    result = func()\n",
    "\n",
    "    return (time.time() - start), result\n",
    "\n",
    "\n",
    "def get_tm_f1(model, train_corpus, X_test, y_train, y_test):\n",
    "    X_train = np.zeros((len(train_corpus), model.num_topics))\n",
    "    for bow_id, bow in enumerate(train_corpus):\n",
    "        for topic_id, factor in model.get_document_topics(bow):\n",
    "            X_train[bow_id, topic_id] = factor\n",
    "\n",
    "    log_reg = LogisticRegressionCV(multi_class='multinomial')\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    pred_labels = log_reg.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, pred_labels, average='micro')\n",
    "\n",
    "\n",
    "def get_sklearn_f1(model, train_corpus, X_test, y_train, y_test):\n",
    "    X_train = model.transform((train_corpus / train_corpus.sum(axis=0)).T)\n",
    "\n",
    "    log_reg = LogisticRegressionCV(multi_class='multinomial')\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    pred_labels = log_reg.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, pred_labels, average='micro')\n",
    "\n",
    "\n",
    "def get_tm_metrics(model, train_corpus, test_corpus, dense_corpus, y_train, y_test):\n",
    "    W = model.get_topics().T\n",
    "    H = np.zeros((model.num_topics, len(test_corpus)))\n",
    "    for bow_id, bow in enumerate(test_corpus):\n",
    "        for topic_id, factor in model.get_document_topics(bow):\n",
    "            H[topic_id, bow_id] = factor\n",
    "\n",
    "    pred_factors = W.dot(H)\n",
    "    pred_factors /= pred_factors.sum(axis=0)\n",
    "\n",
    "    perplexity = get_tm_perplexity(pred_factors, dense_corpus)\n",
    "\n",
    "    l2_norm = get_tm_l2_norm(pred_factors, dense_corpus)\n",
    "\n",
    "    f1 = get_tm_f1(model, train_corpus, H.T, y_train, y_test)\n",
    "\n",
    "    model.normalize = True\n",
    "\n",
    "    coherence = CoherenceModel(\n",
    "        model=model,\n",
    "        corpus=test_corpus,\n",
    "        coherence='u_mass'\n",
    "    ).get_coherence()\n",
    "\n",
    "    model.normalize = False\n",
    "\n",
    "    return dict(\n",
    "        perplexity=perplexity,\n",
    "        coherence=coherence,\n",
    "        l2_norm=l2_norm,\n",
    "        f1=f1,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tm_perplexity(pred_factors, dense_corpus):\n",
    "    return np.exp(-(np.log(pred_factors, where=pred_factors > 0) * dense_corpus).sum() / dense_corpus.sum())\n",
    "\n",
    "\n",
    "def get_tm_l2_norm(pred_factors, dense_corpus):\n",
    "    return np.linalg.norm(dense_corpus / dense_corpus.sum(axis=0) - pred_factors)\n",
    "\n",
    "\n",
    "def get_sklearn_metrics(model, train_corpus, test_corpus, y_train, y_test):\n",
    "    W = model.components_.T\n",
    "    H = model.transform((test_corpus / test_corpus.sum(axis=0)).T).T\n",
    "    pred_factors = W.dot(H)\n",
    "    pred_factors /= pred_factors.sum(axis=0)\n",
    "\n",
    "    perplexity = np.exp(\n",
    "        -(np.log(pred_factors, where=pred_factors > 0) * test_corpus).sum()\n",
    "        / test_corpus.sum()\n",
    "    )\n",
    "\n",
    "    l2_norm = np.linalg.norm(test_corpus / test_corpus.sum(axis=0) - pred_factors)\n",
    "\n",
    "    f1 = get_sklearn_f1(model, train_corpus, H.T, y_train, y_test)\n",
    "\n",
    "    return dict(\n",
    "        perplexity=perplexity,\n",
    "        l2_norm=l2_norm,\n",
    "        f1=f1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-30 16:47:30,583 : INFO : using symmetric alpha at 0.2\n",
      "2019-01-30 16:47:30,583 : INFO : using symmetric eta at 0.2\n",
      "2019-01-30 16:47:30,585 : INFO : using serial LDA version on this node\n",
      "2019-01-30 16:47:30,591 : INFO : running online (multi-pass) LDA training, 5 topics, 5 passes over the supplied corpus of 2819 documents, updating model once every 1000 documents, evaluating perplexity every 2819 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-01-30 16:47:30,592 : INFO : PROGRESS: pass 0, at document #1000/2819\n",
      "2019-01-30 16:47:31,637 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:31,641 : INFO : topic #0 (0.200): 0.006*\"com\" + 0.005*\"new\" + 0.005*\"peopl\" + 0.004*\"space\" + 0.004*\"like\" + 0.004*\"univers\" + 0.004*\"time\" + 0.004*\"nntp\" + 0.004*\"armenian\" + 0.004*\"host\"\n",
      "2019-01-30 16:47:31,643 : INFO : topic #1 (0.200): 0.007*\"com\" + 0.005*\"like\" + 0.005*\"peopl\" + 0.005*\"know\" + 0.004*\"think\" + 0.004*\"time\" + 0.004*\"god\" + 0.004*\"univers\" + 0.004*\"said\" + 0.004*\"host\"\n",
      "2019-01-30 16:47:31,646 : INFO : topic #2 (0.200): 0.005*\"time\" + 0.005*\"like\" + 0.005*\"com\" + 0.005*\"israel\" + 0.005*\"space\" + 0.005*\"univers\" + 0.004*\"peopl\" + 0.004*\"islam\" + 0.004*\"host\" + 0.004*\"isra\"\n",
      "2019-01-30 16:47:31,648 : INFO : topic #3 (0.200): 0.008*\"com\" + 0.006*\"jpeg\" + 0.006*\"imag\" + 0.005*\"nntp\" + 0.005*\"think\" + 0.005*\"file\" + 0.005*\"host\" + 0.004*\"like\" + 0.004*\"univers\" + 0.004*\"graphic\"\n",
      "2019-01-30 16:47:31,650 : INFO : topic #4 (0.200): 0.007*\"peopl\" + 0.006*\"space\" + 0.006*\"com\" + 0.005*\"armenian\" + 0.004*\"know\" + 0.004*\"nasa\" + 0.003*\"right\" + 0.003*\"like\" + 0.003*\"point\" + 0.003*\"time\"\n",
      "2019-01-30 16:47:31,651 : INFO : topic diff=1.686979, rho=1.000000\n",
      "2019-01-30 16:47:31,652 : INFO : PROGRESS: pass 0, at document #2000/2819\n",
      "2019-01-30 16:47:32,678 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:32,683 : INFO : topic #0 (0.200): 0.006*\"com\" + 0.006*\"space\" + 0.005*\"new\" + 0.005*\"armenian\" + 0.005*\"univers\" + 0.004*\"like\" + 0.004*\"peopl\" + 0.004*\"time\" + 0.004*\"nntp\" + 0.004*\"turkish\"\n",
      "2019-01-30 16:47:32,684 : INFO : topic #1 (0.200): 0.007*\"peopl\" + 0.007*\"com\" + 0.006*\"know\" + 0.006*\"like\" + 0.006*\"think\" + 0.005*\"god\" + 0.005*\"said\" + 0.004*\"time\" + 0.004*\"thing\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:32,686 : INFO : topic #2 (0.200): 0.007*\"israel\" + 0.005*\"isra\" + 0.005*\"peopl\" + 0.005*\"islam\" + 0.005*\"like\" + 0.005*\"time\" + 0.005*\"univers\" + 0.005*\"state\" + 0.004*\"god\" + 0.004*\"know\"\n",
      "2019-01-30 16:47:32,688 : INFO : topic #3 (0.200): 0.011*\"imag\" + 0.009*\"com\" + 0.007*\"file\" + 0.006*\"graphic\" + 0.005*\"program\" + 0.005*\"like\" + 0.005*\"host\" + 0.004*\"nntp\" + 0.004*\"univers\" + 0.004*\"us\"\n",
      "2019-01-30 16:47:32,692 : INFO : topic #4 (0.200): 0.011*\"armenian\" + 0.009*\"peopl\" + 0.009*\"space\" + 0.005*\"know\" + 0.005*\"nasa\" + 0.004*\"com\" + 0.004*\"right\" + 0.004*\"like\" + 0.003*\"said\" + 0.003*\"armenia\"\n",
      "2019-01-30 16:47:32,696 : INFO : topic diff=0.848667, rho=0.707107\n",
      "2019-01-30 16:47:33,991 : INFO : -8.075 per-word bound, 269.6 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-01-30 16:47:33,992 : INFO : PROGRESS: pass 0, at document #2819/2819\n",
      "2019-01-30 16:47:34,794 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:34,799 : INFO : topic #0 (0.200): 0.006*\"com\" + 0.006*\"space\" + 0.005*\"new\" + 0.005*\"turkish\" + 0.005*\"bike\" + 0.005*\"univers\" + 0.004*\"year\" + 0.004*\"like\" + 0.004*\"armenian\" + 0.004*\"time\"\n",
      "2019-01-30 16:47:34,801 : INFO : topic #1 (0.200): 0.009*\"com\" + 0.007*\"peopl\" + 0.007*\"like\" + 0.006*\"think\" + 0.006*\"god\" + 0.006*\"know\" + 0.005*\"thing\" + 0.005*\"said\" + 0.004*\"moral\" + 0.004*\"time\"\n",
      "2019-01-30 16:47:34,803 : INFO : topic #2 (0.200): 0.010*\"israel\" + 0.007*\"isra\" + 0.006*\"jew\" + 0.006*\"peopl\" + 0.005*\"state\" + 0.005*\"univers\" + 0.005*\"islam\" + 0.005*\"think\" + 0.005*\"time\" + 0.004*\"arab\"\n",
      "2019-01-30 16:47:34,805 : INFO : topic #3 (0.200): 0.011*\"com\" + 0.009*\"graphic\" + 0.009*\"imag\" + 0.007*\"file\" + 0.006*\"program\" + 0.005*\"host\" + 0.005*\"nntp\" + 0.005*\"softwar\" + 0.005*\"us\" + 0.005*\"like\"\n",
      "2019-01-30 16:47:34,812 : INFO : topic #4 (0.200): 0.014*\"armenian\" + 0.010*\"space\" + 0.008*\"peopl\" + 0.006*\"turkish\" + 0.005*\"launch\" + 0.004*\"nasa\" + 0.004*\"year\" + 0.004*\"turkei\" + 0.004*\"armenia\" + 0.004*\"know\"\n",
      "2019-01-30 16:47:34,814 : INFO : topic diff=0.663294, rho=0.577350\n",
      "2019-01-30 16:47:34,816 : INFO : PROGRESS: pass 1, at document #1000/2819\n",
      "2019-01-30 16:47:35,636 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:35,642 : INFO : topic #0 (0.200): 0.007*\"com\" + 0.007*\"space\" + 0.005*\"new\" + 0.005*\"bike\" + 0.005*\"univers\" + 0.004*\"like\" + 0.004*\"turkish\" + 0.004*\"time\" + 0.004*\"year\" + 0.004*\"nntp\"\n",
      "2019-01-30 16:47:35,644 : INFO : topic #1 (0.200): 0.009*\"com\" + 0.007*\"peopl\" + 0.007*\"like\" + 0.007*\"god\" + 0.006*\"think\" + 0.006*\"know\" + 0.005*\"thing\" + 0.005*\"moral\" + 0.005*\"time\" + 0.004*\"said\"\n",
      "2019-01-30 16:47:35,646 : INFO : topic #2 (0.200): 0.011*\"israel\" + 0.009*\"isra\" + 0.006*\"peopl\" + 0.006*\"jew\" + 0.005*\"arab\" + 0.005*\"islam\" + 0.005*\"think\" + 0.005*\"right\" + 0.005*\"state\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:35,649 : INFO : topic #3 (0.200): 0.012*\"imag\" + 0.010*\"com\" + 0.009*\"file\" + 0.009*\"graphic\" + 0.006*\"program\" + 0.005*\"host\" + 0.005*\"us\" + 0.005*\"jpeg\" + 0.005*\"nntp\" + 0.005*\"univers\"\n",
      "2019-01-30 16:47:35,650 : INFO : topic #4 (0.200): 0.014*\"armenian\" + 0.011*\"space\" + 0.008*\"peopl\" + 0.006*\"nasa\" + 0.006*\"turkish\" + 0.005*\"launch\" + 0.004*\"year\" + 0.004*\"armenia\" + 0.004*\"said\" + 0.004*\"orbit\"\n",
      "2019-01-30 16:47:35,651 : INFO : topic diff=0.431708, rho=0.455535\n",
      "2019-01-30 16:47:35,653 : INFO : PROGRESS: pass 1, at document #2000/2819\n",
      "2019-01-30 16:47:36,515 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:36,519 : INFO : topic #0 (0.200): 0.008*\"space\" + 0.007*\"com\" + 0.006*\"new\" + 0.005*\"bike\" + 0.005*\"univers\" + 0.004*\"like\" + 0.004*\"year\" + 0.004*\"nntp\" + 0.004*\"host\" + 0.004*\"time\"\n",
      "2019-01-30 16:47:36,520 : INFO : topic #1 (0.200): 0.009*\"com\" + 0.008*\"god\" + 0.007*\"peopl\" + 0.007*\"like\" + 0.007*\"think\" + 0.006*\"know\" + 0.005*\"thing\" + 0.005*\"moral\" + 0.005*\"time\" + 0.004*\"said\"\n",
      "2019-01-30 16:47:36,522 : INFO : topic #2 (0.200): 0.011*\"israel\" + 0.009*\"isra\" + 0.007*\"jew\" + 0.006*\"peopl\" + 0.006*\"arab\" + 0.006*\"islam\" + 0.005*\"state\" + 0.005*\"right\" + 0.005*\"think\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:36,524 : INFO : topic #3 (0.200): 0.013*\"imag\" + 0.010*\"com\" + 0.008*\"file\" + 0.008*\"graphic\" + 0.007*\"program\" + 0.005*\"us\" + 0.005*\"host\" + 0.005*\"univers\" + 0.005*\"softwar\" + 0.005*\"nntp\"\n",
      "2019-01-30 16:47:36,526 : INFO : topic #4 (0.200): 0.016*\"armenian\" + 0.010*\"space\" + 0.009*\"peopl\" + 0.005*\"turkish\" + 0.005*\"said\" + 0.005*\"nasa\" + 0.005*\"know\" + 0.004*\"armenia\" + 0.004*\"year\" + 0.004*\"like\"\n",
      "2019-01-30 16:47:36,527 : INFO : topic diff=0.436104, rho=0.455535\n",
      "2019-01-30 16:47:37,625 : INFO : -7.846 per-word bound, 230.1 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-01-30 16:47:37,626 : INFO : PROGRESS: pass 1, at document #2819/2819\n",
      "2019-01-30 16:47:38,144 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:38,147 : INFO : topic #0 (0.200): 0.008*\"space\" + 0.007*\"com\" + 0.006*\"bike\" + 0.006*\"new\" + 0.005*\"univers\" + 0.005*\"year\" + 0.004*\"like\" + 0.004*\"orbit\" + 0.004*\"dod\" + 0.004*\"host\"\n",
      "2019-01-30 16:47:38,149 : INFO : topic #1 (0.200): 0.010*\"com\" + 0.008*\"god\" + 0.007*\"peopl\" + 0.007*\"like\" + 0.007*\"think\" + 0.006*\"know\" + 0.005*\"thing\" + 0.005*\"moral\" + 0.005*\"time\" + 0.004*\"said\"\n",
      "2019-01-30 16:47:38,150 : INFO : topic #2 (0.200): 0.012*\"israel\" + 0.009*\"isra\" + 0.008*\"jew\" + 0.007*\"peopl\" + 0.006*\"arab\" + 0.005*\"state\" + 0.005*\"islam\" + 0.005*\"right\" + 0.005*\"think\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:38,153 : INFO : topic #3 (0.200): 0.011*\"imag\" + 0.010*\"com\" + 0.010*\"graphic\" + 0.008*\"file\" + 0.007*\"program\" + 0.006*\"softwar\" + 0.005*\"host\" + 0.005*\"us\" + 0.005*\"nntp\" + 0.005*\"univers\"\n",
      "2019-01-30 16:47:38,156 : INFO : topic #4 (0.200): 0.017*\"armenian\" + 0.009*\"turkish\" + 0.009*\"space\" + 0.008*\"peopl\" + 0.005*\"said\" + 0.005*\"launch\" + 0.005*\"armenia\" + 0.005*\"year\" + 0.005*\"nasa\" + 0.004*\"turkei\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-30 16:47:38,156 : INFO : topic diff=0.423402, rho=0.455535\n",
      "2019-01-30 16:47:38,157 : INFO : PROGRESS: pass 2, at document #1000/2819\n",
      "2019-01-30 16:47:38,810 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:38,814 : INFO : topic #0 (0.200): 0.009*\"space\" + 0.007*\"com\" + 0.006*\"bike\" + 0.006*\"new\" + 0.005*\"univers\" + 0.005*\"orbit\" + 0.004*\"nasa\" + 0.004*\"year\" + 0.004*\"like\" + 0.004*\"time\"\n",
      "2019-01-30 16:47:38,815 : INFO : topic #1 (0.200): 0.010*\"com\" + 0.009*\"god\" + 0.007*\"peopl\" + 0.007*\"like\" + 0.007*\"think\" + 0.006*\"know\" + 0.006*\"thing\" + 0.006*\"moral\" + 0.005*\"time\" + 0.005*\"atheist\"\n",
      "2019-01-30 16:47:38,817 : INFO : topic #2 (0.200): 0.012*\"israel\" + 0.010*\"isra\" + 0.007*\"jew\" + 0.007*\"peopl\" + 0.006*\"arab\" + 0.006*\"islam\" + 0.005*\"right\" + 0.005*\"think\" + 0.005*\"state\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:38,818 : INFO : topic #3 (0.200): 0.013*\"imag\" + 0.009*\"file\" + 0.009*\"graphic\" + 0.009*\"com\" + 0.007*\"program\" + 0.006*\"us\" + 0.006*\"host\" + 0.005*\"univers\" + 0.005*\"jpeg\" + 0.005*\"nntp\"\n",
      "2019-01-30 16:47:38,820 : INFO : topic #4 (0.200): 0.017*\"armenian\" + 0.009*\"turkish\" + 0.008*\"peopl\" + 0.008*\"space\" + 0.005*\"said\" + 0.005*\"nasa\" + 0.005*\"armenia\" + 0.005*\"year\" + 0.004*\"launch\" + 0.004*\"turkei\"\n",
      "2019-01-30 16:47:38,821 : INFO : topic diff=0.333964, rho=0.414549\n",
      "2019-01-30 16:47:38,822 : INFO : PROGRESS: pass 2, at document #2000/2819\n",
      "2019-01-30 16:47:39,468 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:39,472 : INFO : topic #0 (0.200): 0.011*\"space\" + 0.008*\"com\" + 0.006*\"bike\" + 0.006*\"new\" + 0.005*\"nasa\" + 0.005*\"univers\" + 0.005*\"orbit\" + 0.004*\"year\" + 0.004*\"like\" + 0.004*\"host\"\n",
      "2019-01-30 16:47:39,474 : INFO : topic #1 (0.200): 0.010*\"com\" + 0.010*\"god\" + 0.007*\"peopl\" + 0.007*\"like\" + 0.007*\"think\" + 0.006*\"know\" + 0.006*\"thing\" + 0.005*\"moral\" + 0.005*\"time\" + 0.004*\"atheist\"\n",
      "2019-01-30 16:47:39,477 : INFO : topic #2 (0.200): 0.012*\"israel\" + 0.010*\"isra\" + 0.008*\"jew\" + 0.007*\"arab\" + 0.007*\"peopl\" + 0.006*\"islam\" + 0.006*\"state\" + 0.005*\"right\" + 0.005*\"think\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:39,479 : INFO : topic #3 (0.200): 0.014*\"imag\" + 0.009*\"file\" + 0.009*\"graphic\" + 0.009*\"com\" + 0.007*\"program\" + 0.006*\"us\" + 0.006*\"univers\" + 0.005*\"softwar\" + 0.005*\"host\" + 0.005*\"nntp\"\n",
      "2019-01-30 16:47:39,481 : INFO : topic #4 (0.200): 0.018*\"armenian\" + 0.010*\"peopl\" + 0.008*\"turkish\" + 0.007*\"space\" + 0.006*\"said\" + 0.005*\"know\" + 0.005*\"armenia\" + 0.004*\"like\" + 0.004*\"year\" + 0.004*\"nasa\"\n",
      "2019-01-30 16:47:39,482 : INFO : topic diff=0.334136, rho=0.414549\n",
      "2019-01-30 16:47:40,337 : INFO : -7.786 per-word bound, 220.6 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-01-30 16:47:40,337 : INFO : PROGRESS: pass 2, at document #2819/2819\n",
      "2019-01-30 16:47:40,966 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:40,972 : INFO : topic #0 (0.200): 0.011*\"space\" + 0.008*\"com\" + 0.007*\"bike\" + 0.006*\"new\" + 0.005*\"univers\" + 0.005*\"orbit\" + 0.005*\"nasa\" + 0.005*\"year\" + 0.004*\"like\" + 0.004*\"satellit\"\n",
      "2019-01-30 16:47:40,973 : INFO : topic #1 (0.200): 0.011*\"com\" + 0.010*\"god\" + 0.008*\"peopl\" + 0.007*\"think\" + 0.007*\"like\" + 0.006*\"thing\" + 0.006*\"know\" + 0.005*\"moral\" + 0.005*\"time\" + 0.004*\"believ\"\n",
      "2019-01-30 16:47:40,975 : INFO : topic #2 (0.200): 0.012*\"israel\" + 0.010*\"isra\" + 0.009*\"jew\" + 0.007*\"peopl\" + 0.007*\"arab\" + 0.006*\"state\" + 0.005*\"islam\" + 0.005*\"right\" + 0.005*\"think\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:40,980 : INFO : topic #3 (0.200): 0.012*\"imag\" + 0.010*\"graphic\" + 0.009*\"com\" + 0.009*\"file\" + 0.007*\"program\" + 0.006*\"softwar\" + 0.006*\"us\" + 0.005*\"univers\" + 0.005*\"host\" + 0.005*\"mail\"\n",
      "2019-01-30 16:47:40,981 : INFO : topic #4 (0.200): 0.018*\"armenian\" + 0.011*\"turkish\" + 0.009*\"peopl\" + 0.006*\"said\" + 0.006*\"space\" + 0.005*\"turkei\" + 0.005*\"armenia\" + 0.005*\"turk\" + 0.005*\"year\" + 0.005*\"know\"\n",
      "2019-01-30 16:47:40,982 : INFO : topic diff=0.321527, rho=0.414549\n",
      "2019-01-30 16:47:40,985 : INFO : PROGRESS: pass 3, at document #1000/2819\n",
      "2019-01-30 16:47:41,704 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:41,709 : INFO : topic #0 (0.200): 0.012*\"space\" + 0.008*\"com\" + 0.007*\"bike\" + 0.006*\"orbit\" + 0.006*\"nasa\" + 0.006*\"new\" + 0.005*\"univers\" + 0.005*\"year\" + 0.004*\"like\" + 0.004*\"host\"\n",
      "2019-01-30 16:47:41,710 : INFO : topic #1 (0.200): 0.011*\"com\" + 0.010*\"god\" + 0.008*\"peopl\" + 0.007*\"like\" + 0.007*\"think\" + 0.006*\"thing\" + 0.006*\"know\" + 0.006*\"moral\" + 0.005*\"time\" + 0.005*\"atheist\"\n",
      "2019-01-30 16:47:41,712 : INFO : topic #2 (0.200): 0.012*\"israel\" + 0.011*\"isra\" + 0.008*\"jew\" + 0.007*\"arab\" + 0.007*\"peopl\" + 0.006*\"islam\" + 0.006*\"right\" + 0.005*\"state\" + 0.005*\"think\" + 0.004*\"univers\"\n",
      "2019-01-30 16:47:41,714 : INFO : topic #3 (0.200): 0.013*\"imag\" + 0.010*\"file\" + 0.009*\"graphic\" + 0.008*\"com\" + 0.007*\"program\" + 0.006*\"us\" + 0.006*\"univers\" + 0.005*\"host\" + 0.005*\"jpeg\" + 0.005*\"softwar\"\n",
      "2019-01-30 16:47:41,715 : INFO : topic #4 (0.200): 0.018*\"armenian\" + 0.010*\"turkish\" + 0.009*\"peopl\" + 0.006*\"said\" + 0.005*\"armenia\" + 0.005*\"space\" + 0.005*\"turk\" + 0.005*\"turkei\" + 0.004*\"year\" + 0.004*\"know\"\n",
      "2019-01-30 16:47:41,716 : INFO : topic diff=0.255652, rho=0.382948\n",
      "2019-01-30 16:47:41,717 : INFO : PROGRESS: pass 3, at document #2000/2819\n",
      "2019-01-30 16:47:42,376 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:42,381 : INFO : topic #0 (0.200): 0.013*\"space\" + 0.008*\"com\" + 0.007*\"nasa\" + 0.006*\"bike\" + 0.006*\"new\" + 0.006*\"orbit\" + 0.005*\"univers\" + 0.004*\"year\" + 0.004*\"host\" + 0.004*\"nntp\"\n",
      "2019-01-30 16:47:42,383 : INFO : topic #1 (0.200): 0.011*\"god\" + 0.010*\"com\" + 0.008*\"peopl\" + 0.007*\"think\" + 0.007*\"like\" + 0.006*\"know\" + 0.006*\"thing\" + 0.005*\"moral\" + 0.005*\"time\" + 0.005*\"believ\"\n",
      "2019-01-30 16:47:42,385 : INFO : topic #2 (0.200): 0.012*\"israel\" + 0.011*\"isra\" + 0.008*\"jew\" + 0.007*\"arab\" + 0.007*\"peopl\" + 0.006*\"islam\" + 0.006*\"state\" + 0.006*\"right\" + 0.005*\"think\" + 0.004*\"jewish\"\n",
      "2019-01-30 16:47:42,390 : INFO : topic #3 (0.200): 0.014*\"imag\" + 0.009*\"file\" + 0.009*\"graphic\" + 0.008*\"com\" + 0.007*\"program\" + 0.006*\"us\" + 0.006*\"univers\" + 0.006*\"softwar\" + 0.005*\"host\" + 0.005*\"nntp\"\n",
      "2019-01-30 16:47:42,392 : INFO : topic #4 (0.200): 0.019*\"armenian\" + 0.011*\"peopl\" + 0.009*\"turkish\" + 0.007*\"said\" + 0.006*\"know\" + 0.005*\"armenia\" + 0.005*\"turk\" + 0.005*\"like\" + 0.004*\"year\" + 0.004*\"turkei\"\n",
      "2019-01-30 16:47:42,393 : INFO : topic diff=0.256253, rho=0.382948\n",
      "2019-01-30 16:47:43,362 : INFO : -7.754 per-word bound, 215.8 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-01-30 16:47:43,362 : INFO : PROGRESS: pass 3, at document #2819/2819\n",
      "2019-01-30 16:47:43,855 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:43,859 : INFO : topic #0 (0.200): 0.013*\"space\" + 0.008*\"com\" + 0.007*\"bike\" + 0.006*\"nasa\" + 0.006*\"new\" + 0.005*\"orbit\" + 0.005*\"year\" + 0.005*\"univers\" + 0.005*\"launch\" + 0.004*\"like\"\n",
      "2019-01-30 16:47:43,862 : INFO : topic #1 (0.200): 0.011*\"com\" + 0.010*\"god\" + 0.008*\"peopl\" + 0.007*\"think\" + 0.007*\"like\" + 0.006*\"thing\" + 0.006*\"know\" + 0.005*\"moral\" + 0.005*\"believ\" + 0.005*\"time\"\n",
      "2019-01-30 16:47:43,863 : INFO : topic #2 (0.200): 0.013*\"israel\" + 0.010*\"isra\" + 0.009*\"jew\" + 0.007*\"arab\" + 0.007*\"peopl\" + 0.006*\"state\" + 0.006*\"islam\" + 0.006*\"right\" + 0.005*\"think\" + 0.004*\"war\"\n",
      "2019-01-30 16:47:43,865 : INFO : topic #3 (0.200): 0.012*\"imag\" + 0.010*\"graphic\" + 0.009*\"file\" + 0.008*\"com\" + 0.008*\"program\" + 0.006*\"softwar\" + 0.006*\"us\" + 0.006*\"univers\" + 0.005*\"host\" + 0.005*\"mail\"\n",
      "2019-01-30 16:47:43,866 : INFO : topic #4 (0.200): 0.019*\"armenian\" + 0.012*\"turkish\" + 0.010*\"peopl\" + 0.007*\"said\" + 0.006*\"turkei\" + 0.005*\"armenia\" + 0.005*\"turk\" + 0.005*\"know\" + 0.004*\"year\" + 0.004*\"like\"\n",
      "2019-01-30 16:47:43,868 : INFO : topic diff=0.249832, rho=0.382948\n",
      "2019-01-30 16:47:43,871 : INFO : PROGRESS: pass 4, at document #1000/2819\n",
      "2019-01-30 16:47:44,461 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:44,465 : INFO : topic #0 (0.200): 0.013*\"space\" + 0.008*\"com\" + 0.007*\"bike\" + 0.007*\"nasa\" + 0.006*\"orbit\" + 0.005*\"new\" + 0.005*\"year\" + 0.005*\"univers\" + 0.005*\"launch\" + 0.004*\"host\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-30 16:47:44,467 : INFO : topic #1 (0.200): 0.011*\"com\" + 0.011*\"god\" + 0.008*\"peopl\" + 0.007*\"think\" + 0.007*\"like\" + 0.006*\"thing\" + 0.006*\"know\" + 0.006*\"moral\" + 0.005*\"atheist\" + 0.005*\"time\"\n",
      "2019-01-30 16:47:44,469 : INFO : topic #2 (0.200): 0.013*\"israel\" + 0.011*\"isra\" + 0.009*\"jew\" + 0.007*\"arab\" + 0.007*\"peopl\" + 0.006*\"islam\" + 0.006*\"right\" + 0.005*\"state\" + 0.005*\"think\" + 0.004*\"peac\"\n",
      "2019-01-30 16:47:44,473 : INFO : topic #3 (0.200): 0.014*\"imag\" + 0.010*\"file\" + 0.010*\"graphic\" + 0.008*\"com\" + 0.008*\"program\" + 0.006*\"us\" + 0.006*\"univers\" + 0.005*\"softwar\" + 0.005*\"host\" + 0.005*\"jpeg\"\n",
      "2019-01-30 16:47:44,475 : INFO : topic #4 (0.200): 0.019*\"armenian\" + 0.011*\"turkish\" + 0.010*\"peopl\" + 0.006*\"said\" + 0.006*\"armenia\" + 0.006*\"turk\" + 0.005*\"turkei\" + 0.005*\"know\" + 0.004*\"greek\" + 0.004*\"year\"\n",
      "2019-01-30 16:47:44,476 : INFO : topic diff=0.204471, rho=0.357622\n",
      "2019-01-30 16:47:44,478 : INFO : PROGRESS: pass 4, at document #2000/2819\n",
      "2019-01-30 16:47:45,013 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:45,017 : INFO : topic #0 (0.200): 0.014*\"space\" + 0.008*\"com\" + 0.008*\"nasa\" + 0.007*\"bike\" + 0.006*\"orbit\" + 0.006*\"new\" + 0.005*\"univers\" + 0.005*\"year\" + 0.004*\"host\" + 0.004*\"nntp\"\n",
      "2019-01-30 16:47:45,018 : INFO : topic #1 (0.200): 0.011*\"god\" + 0.010*\"com\" + 0.008*\"peopl\" + 0.007*\"think\" + 0.007*\"like\" + 0.006*\"thing\" + 0.006*\"know\" + 0.005*\"moral\" + 0.005*\"believ\" + 0.005*\"atheist\"\n",
      "2019-01-30 16:47:45,019 : INFO : topic #2 (0.200): 0.012*\"israel\" + 0.011*\"isra\" + 0.009*\"jew\" + 0.008*\"arab\" + 0.007*\"peopl\" + 0.006*\"islam\" + 0.006*\"right\" + 0.006*\"state\" + 0.005*\"think\" + 0.004*\"jewish\"\n",
      "2019-01-30 16:47:45,020 : INFO : topic #3 (0.200): 0.014*\"imag\" + 0.010*\"file\" + 0.009*\"graphic\" + 0.008*\"program\" + 0.007*\"com\" + 0.006*\"univers\" + 0.006*\"us\" + 0.006*\"softwar\" + 0.005*\"host\" + 0.005*\"need\"\n",
      "2019-01-30 16:47:45,021 : INFO : topic #4 (0.200): 0.019*\"armenian\" + 0.011*\"peopl\" + 0.010*\"turkish\" + 0.007*\"said\" + 0.006*\"know\" + 0.006*\"armenia\" + 0.005*\"turk\" + 0.005*\"like\" + 0.005*\"turkei\" + 0.004*\"time\"\n",
      "2019-01-30 16:47:45,023 : INFO : topic diff=0.206189, rho=0.357622\n",
      "2019-01-30 16:47:45,799 : INFO : -7.735 per-word bound, 213.0 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-01-30 16:47:45,800 : INFO : PROGRESS: pass 4, at document #2819/2819\n",
      "2019-01-30 16:47:46,256 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-01-30 16:47:46,259 : INFO : topic #0 (0.200): 0.014*\"space\" + 0.008*\"com\" + 0.007*\"bike\" + 0.007*\"nasa\" + 0.005*\"new\" + 0.005*\"orbit\" + 0.005*\"launch\" + 0.005*\"year\" + 0.005*\"univers\" + 0.004*\"like\"\n",
      "2019-01-30 16:47:46,261 : INFO : topic #1 (0.200): 0.011*\"god\" + 0.011*\"com\" + 0.008*\"peopl\" + 0.008*\"think\" + 0.007*\"like\" + 0.006*\"thing\" + 0.006*\"know\" + 0.005*\"moral\" + 0.005*\"believ\" + 0.005*\"time\"\n",
      "2019-01-30 16:47:46,262 : INFO : topic #2 (0.200): 0.013*\"israel\" + 0.011*\"isra\" + 0.010*\"jew\" + 0.007*\"arab\" + 0.007*\"peopl\" + 0.006*\"state\" + 0.006*\"islam\" + 0.006*\"right\" + 0.005*\"think\" + 0.004*\"jewish\"\n",
      "2019-01-30 16:47:46,264 : INFO : topic #3 (0.200): 0.012*\"imag\" + 0.010*\"graphic\" + 0.010*\"file\" + 0.008*\"com\" + 0.008*\"program\" + 0.006*\"softwar\" + 0.006*\"univers\" + 0.006*\"us\" + 0.005*\"mail\" + 0.005*\"host\"\n",
      "2019-01-30 16:47:46,266 : INFO : topic #4 (0.200): 0.020*\"armenian\" + 0.012*\"turkish\" + 0.010*\"peopl\" + 0.007*\"said\" + 0.006*\"turkei\" + 0.006*\"armenia\" + 0.006*\"turk\" + 0.005*\"know\" + 0.004*\"greek\" + 0.004*\"year\"\n",
      "2019-01-30 16:47:46,267 : INFO : topic diff=0.203499, rho=0.357622\n",
      "2019-01-30 16:47:51,327 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "2019-01-30 16:48:10,582 : INFO : Loss: 1.0280021673693736\n",
      "2019-01-30 16:48:10,786 : INFO : Loss: 0.9805869534381415\n",
      "2019-01-30 16:48:16,788 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    }
   ],
   "source": [
    "tm_metrics = pd.DataFrame()\n",
    "\n",
    "train_dense_corpus = matutils.corpus2dense(train_corpus, len(dictionary))\n",
    "test_dense_corpus = matutils.corpus2dense(test_corpus, len(dictionary))\n",
    "\n",
    "trainset_target = [doc['target'] for doc in trainset]\n",
    "testset_target = [doc['target'] for doc in testset]\n",
    "\n",
    "# LDA metrics\n",
    "row = dict()\n",
    "row['model'] = 'lda'\n",
    "row['train_time'], lda = get_execution_time(\n",
    "    lambda: LdaModel(**fixed_params)\n",
    ")\n",
    "row.update(get_tm_metrics(\n",
    "    lda, train_corpus, test_corpus, test_dense_corpus, trainset_target, testset_target,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "\n",
    "# Sklearn NMF metrics\n",
    "row = dict()\n",
    "row['model'] = 'sklearn_nmf'\n",
    "sklearn_nmf = SklearnNmf(n_components=5, tol=1e-5, max_iter=int(1e9), random_state=42)\n",
    "row['train_time'], sklearn_nmf = get_execution_time(\n",
    "    lambda: sklearn_nmf.fit((train_dense_corpus / train_dense_corpus.sum(axis=0)).T)\n",
    ")\n",
    "row.update(get_sklearn_metrics(\n",
    "    sklearn_nmf, train_dense_corpus, test_dense_corpus, trainset_target, testset_target,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "\n",
    "row = dict()\n",
    "row['model'] = 'gensim_nmf'\n",
    "row['train_time'], model = get_execution_time(\n",
    "    lambda: GensimNmf(\n",
    "        normalize=False,\n",
    "        **fixed_params\n",
    "    )\n",
    ")\n",
    "row.update(get_tm_metrics(\n",
    "    model, train_corpus, test_corpus, test_dense_corpus, trainset_target, testset_target,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "tm_metrics.replace(np.nan, '-', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coherence</th>\n",
       "      <th>f1</th>\n",
       "      <th>l2_norm</th>\n",
       "      <th>model</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>0.696695</td>\n",
       "      <td>6.929583</td>\n",
       "      <td>sklearn_nmf</td>\n",
       "      <td>2404.189918</td>\n",
       "      <td>14.110252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.70539</td>\n",
       "      <td>0.715352</td>\n",
       "      <td>7.061342</td>\n",
       "      <td>gensim_nmf</td>\n",
       "      <td>2475.979773</td>\n",
       "      <td>0.719364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.75565</td>\n",
       "      <td>0.765458</td>\n",
       "      <td>7.002725</td>\n",
       "      <td>lda</td>\n",
       "      <td>1939.575705</td>\n",
       "      <td>15.685964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coherence        f1   l2_norm        model   perplexity  train_time\n",
       "1         -  0.696695  6.929583  sklearn_nmf  2404.189918   14.110252\n",
       "2  -1.70539  0.715352  7.061342   gensim_nmf  2475.979773    0.719364\n",
       "0  -1.75565  0.765458  7.002725          lda  1939.575705   15.685964"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_metrics.sort_values('f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main insights\n",
    "\n",
    "- Gensim NMF is **ridiculously** fast and leaves LDA and Sklearn far behind in terms of training time\n",
    "- Gensim NMF beats sklearn NMF implementation on f1 metric, though not on the l2 norm and perplexity\n",
    "- Gensim NMF beats LDA on coherence, but LDA is still better on perplexity and l2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faces Dataset Decomposition + Gensim NMF\n",
    "\n",
    "NMF algorithm works not only with texts, but with all kinds of stuff!\n",
    "\n",
    "Let's compare our model with the other factorization algorithms and check out the results!\n",
    "\n",
    "To do that we'll patch sklearn's [Faces Dataset Decomposition](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn wrapper\n",
    "Let's create a wrapper to compare Gensim NMF with the other factorizations on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "\n",
    "class NmfWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bow_matrix, **kwargs):\n",
    "        self.corpus = sparse.csc.csc_matrix(bow_matrix.T)\n",
    "        self.nmf = GensimNmf(**kwargs)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.nmf.update(self.corpus)\n",
    "\n",
    "    @property\n",
    "    def components_(self):\n",
    "        return self.nmf.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified FDD notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Faces dataset decompositions\n",
      "============================\n",
      "\n",
      "This example applies to :ref:`olivetti_faces` different unsupervised\n",
      "matrix decomposition (dimension reduction) methods from the module\n",
      ":py:mod:`sklearn.decomposition` (see the documentation chapter\n",
      ":ref:`decompositions`) .\n",
      "\n",
      "\n",
      "Dataset consists of 400 faces\n",
      "Extracting the top 6 Eigenfaces - PCA using randomized SVD...\n",
      "done in 0.176s\n",
      "Extracting the top 6 Non-negative components - NMF (Sklearn)...\n",
      "done in 0.859s\n",
      "Extracting the top 6 Non-negative components - NMF (Gensim)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-30 16:48:19,089 : INFO : Loss: 1.0009276069379216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.590s\n",
      "Extracting the top 6 Independent components - FastICA...\n",
      "done in 0.296s\n",
      "Extracting the top 6 Sparse comp. - MiniBatchSparsePCA...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================\n",
    "Faces dataset decompositions\n",
    "============================\n",
    "\n",
    "This example applies to :ref:`olivetti_faces` different unsupervised\n",
    "matrix decomposition (dimension reduction) methods from the module\n",
    ":py:mod:`sklearn.decomposition` (see the documentation chapter\n",
    ":ref:`decompositions`) .\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Authors: Vlad Niculae, Alexandre Gramfort\n",
    "# License: BSD 3 claus\n",
    "\n",
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "# #############################################################################\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# List of the different estimators, whether to center and transpose the\n",
    "# problem, and whether the transformer uses the clustering API.\n",
    "estimators = [\n",
    "    ('Eigenfaces - PCA using randomized SVD',\n",
    "     decomposition.PCA(n_components=n_components, svd_solver='randomized',\n",
    "                       whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Non-negative components - NMF (Sklearn)',\n",
    "     decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),\n",
    "     False),\n",
    "\n",
    "    ('Non-negative components - NMF (Gensim)',\n",
    "     NmfWrapper(\n",
    "         bow_matrix=faces.T,\n",
    "         chunksize=2,\n",
    "         eval_every=400,\n",
    "         passes=1,\n",
    "         id2word={idx: idx for idx in range(faces.shape[1])},\n",
    "         num_topics=n_components,\n",
    "         minimum_probability=0,\n",
    "         random_state=42,\n",
    "     ),\n",
    "     False),\n",
    "\n",
    "    ('Independent components - FastICA',\n",
    "     decomposition.FastICA(n_components=n_components, whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Sparse comp. - MiniBatchSparsePCA',\n",
    "     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,\n",
    "                                      n_iter=100, batch_size=3,\n",
    "                                      random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('MiniBatchDictionaryLearning',\n",
    "     decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n",
    "                                               n_iter=50, batch_size=3,\n",
    "                                               random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Cluster centers - MiniBatchKMeans',\n",
    "     MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,\n",
    "                     max_iter=50, random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Factor Analysis components - FA',\n",
    "     decomposition.FactorAnalysis(n_components=n_components, max_iter=2),\n",
    "     True),\n",
    "]\n",
    "\n",
    "# #############################################################################\n",
    "# Plot a sample of the input data\n",
    "\n",
    "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])\n",
    "\n",
    "# #############################################################################\n",
    "# Do the estimation and plot it\n",
    "\n",
    "for name, estimator, center in estimators:\n",
    "    print(\"Extracting the top %d %s...\" % (n_components, name))\n",
    "    t0 = time.time()\n",
    "    data = faces\n",
    "    if center:\n",
    "        data = faces_centered\n",
    "    estimator.fit(data)\n",
    "    train_time = (time.time() - t0)\n",
    "    print(\"done in %0.3fs\" % train_time)\n",
    "    if hasattr(estimator, 'cluster_centers_'):\n",
    "        components_ = estimator.cluster_centers_\n",
    "    else:\n",
    "        components_ = estimator.components_\n",
    "\n",
    "    # Plot an image representing the pixelwise variance provided by the\n",
    "    # estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,\n",
    "    # via the PCA decomposition, also provides a scalar noise_variance_\n",
    "    # (the mean of pixelwise variance) that cannot be displayed as an image\n",
    "    # so we skip it.\n",
    "    if (hasattr(estimator, 'noise_variance_') and\n",
    "            estimator.noise_variance_.ndim > 0):  # Skip the Eigenfaces case\n",
    "        plot_gallery(\"Pixelwise variance\",\n",
    "                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n",
    "                     n_row=1)\n",
    "    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n",
    "                 components_[:n_components])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Gensim NMF implementation works as fast as Sklearn NMF and achieves comparable quality, even though it's not optimised for dense matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Gensim NMF is an extremely fast and memory-optimized model, and should be used whenever your system resources are too scarse for the task or when you want to try something different from LDA."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
