{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Online Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks explains basic ideas behind NMF implementation, training examples and use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Matrix Factorizations** are useful for many things: recomendation systems, bi-clustering, image compression and, in particular, topic modeling.\n",
    "\n",
    "Why **Non-Negative**? It makes the problem more strict and allows us to apply some optimizations.\n",
    "\n",
    "Why **Online**? Because corpora are large and RAM is limited. Online NMF can learn topics iteratively.\n",
    "\n",
    "This particular implementation is based on [this paper](https://arxiv.org/abs/1604.02634).\n",
    "\n",
    "The main attributes are following:\n",
    "\n",
    "- W is a word-topic matrix\n",
    "- h is a topic-document matrix\n",
    "- v is an input corpus batch, word-document matrix\n",
    "- A, B - matrices that accumulate information from every consecutive chunk. A = h.dot(ht), B = v.dot(ht).\n",
    "\n",
    "The idea of the algorithm is as follows:\n",
    "\n",
    "```\n",
    "    Initialize W, A and B matrices\n",
    "\n",
    "    Input corpus\n",
    "    Split corpus to batches\n",
    "\n",
    "    for v in batches:\n",
    "        infer h:\n",
    "            do coordinate gradient descent step to find h that minimizes (v - Wh) l2 norm\n",
    "\n",
    "            bound h so that it is non-negative\n",
    "\n",
    "        update A and B:\n",
    "            A = h.dot(ht)\n",
    "            B = v.dot(ht)\n",
    "\n",
    "        update W:\n",
    "            do gradient descent step to find W that minimizes 0.5*trace(WtWA) - trace(WtB) l2 norm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in this tutorial?\n",
    "\n",
    "- Basic training example\n",
    "- Comparison with alternative models (LDA and Sklearn NMF)\n",
    "- Non-standart application (image decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anotherbugmaster/.virtualenvs/gensim/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from numpy.random import RandomState\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition.nmf import NMF as SklearnNmf\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim import matutils\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim.models.nmf import Nmf as GensimNmf\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = api.load('20-newsgroups')\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'rec.motorcycles',\n",
    "    'talk.politics.mideast',\n",
    "    'sci.space'\n",
    "]\n",
    "\n",
    "categories = {\n",
    "    name: idx\n",
    "    for idx, name\n",
    "    in enumerate(categories)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = RandomState(42)\n",
    "\n",
    "trainset = np.array([\n",
    "    {\n",
    "        'data': doc['data'],\n",
    "        'target': categories[doc['topic']],\n",
    "    }\n",
    "    for doc\n",
    "    in newsgroups\n",
    "    if doc['topic'] in categories\n",
    "    and doc['set'] == 'train'\n",
    "])\n",
    "random_state.shuffle(trainset)\n",
    "\n",
    "testset = np.array([\n",
    "    {\n",
    "        'data': doc['data'],\n",
    "        'target': categories[doc['topic']],\n",
    "    }\n",
    "    for doc\n",
    "    in newsgroups\n",
    "    if doc['topic'] in categories\n",
    "    and doc['set'] == 'test'\n",
    "])\n",
    "random_state.shuffle(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = [preprocess_string(doc['data']) for doc in trainset]\n",
    "test_documents = [preprocess_string(doc['data']) for doc in testset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(train_documents)\n",
    "dictionary.filter_extremes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [\n",
    "    dictionary.doc2bow(document)\n",
    "    for document\n",
    "    in train_documents\n",
    "]\n",
    "\n",
    "test_corpus = [\n",
    "    dictionary.doc2bow(document)\n",
    "    for document\n",
    "    in test_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The API works in the way similar to [Gensim.models.LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html).\n",
    "\n",
    "Special parameters:\n",
    "\n",
    "- `kappa` float, optional\n",
    "\n",
    "    Gradient descent step size.\n",
    "    \n",
    "    Larger value makes the model train faster, but could lead to non-convergence if set too large.\n",
    "    \n",
    "    \n",
    "- `w_max_iter` int, optional\n",
    "\n",
    "    Maximum number of iterations to train W per each batch.\n",
    "    \n",
    "    \n",
    "- `w_stop_condition` float, optional\n",
    "\n",
    "    If error difference gets less than that, training of ``W`` stops for the current batch.\n",
    "    \n",
    "    \n",
    "- `h_r_max_iter` int, optional\n",
    "\n",
    "    Maximum number of iterations to train h per each batch.\n",
    "    \n",
    "    \n",
    "- `h_r_stop_condition` float\n",
    "\n",
    "    If error difference gets less than that, training of ``h`` stops for the current batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nmf = GensimNmf(\n",
    "    corpus=train_corpus,\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    chunksize=1000,\n",
    "    passes=5,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0,\n",
    "    random_state=0,\n",
    "    kappa=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence\n",
    "\n",
    "Here's a [description of what coherence is](http://qpleple.com/topic-coherence-to-evaluate-topic-models/). Basically it measures how often do most frequent tokens from each topic co-occur in one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoherenceModel(\n",
    "    model=nmf,\n",
    "    corpus=test_corpus,\n",
    "    coherence='u_mass'\n",
    ").get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "[Perplexity](http://qpleple.com/perplexity-to-evaluate-topic-models/) is basically a degree of uncertainty of the model, i.e. how probable it is to observe a particular set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-nmf.log_perplexity(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Document topics inference\n",
    "\n",
    "Let's get some news and infer a topic vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testset[0]['data'])\n",
    "print('=' * 100)\n",
    "print(\"Topics: {}\".format(nmf[test_corpus[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word topic inference\n",
    "\n",
    "Here's an example of topic distribution inference for a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "word = dictionary[0]\n",
    "print(\"Word: {}\".format(word))\n",
    "print(\"Topics: {}\".format(nmf.get_term_topics(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density is a fraction of non-zero elements in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def density(matrix):\n",
    "    return (matrix > 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term-topic matrix of shape `(words, topics)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Density: {}\".format(density(nmf._W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic-document matrix for the last batch of shape `(topics, batch)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Density: {}\".format(density(nmf._h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals matrix of the last batch of shape `(words, batch)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim NMF vs Sklearn NMF vs Gensim LDA\n",
    "\n",
    "We'll run all the models on the [20newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) dataset, which has texts and labels for them.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- `train time` - time to train a model\n",
    "- `perplexity` - perplexity score. Another usual TM metric\n",
    "- `coherence` - coherence score (not defined for sklearn NMF). Classic metric for topic models.\n",
    "- `l2_norm` - l2 norm of `v - Wh`\n",
    "- `f1` - f1 on the task of news topic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    corpus=train_corpus,\n",
    "    chunksize=1000,\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    passes=5,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_execution_time(func):\n",
    "    start = time.time()\n",
    "    result = func()\n",
    "    \n",
    "    elapsed_time = pd.to_timedelta(time.time() - start, unit='s').round('s')\n",
    "\n",
    "    return elapsed_time, result\n",
    "\n",
    "\n",
    "def get_tm_f1(model, train_corpus, X_test, y_train, y_test):\n",
    "    X_train = np.zeros((len(train_corpus), model.num_topics))\n",
    "    for bow_id, bow in enumerate(train_corpus):\n",
    "        for topic_id, factor in model.get_document_topics(bow):\n",
    "            X_train[bow_id, topic_id] = factor\n",
    "\n",
    "    log_reg = LogisticRegressionCV(multi_class='multinomial')\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    pred_labels = log_reg.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, pred_labels, average='micro')\n",
    "\n",
    "\n",
    "def get_sklearn_f1(model, train_corpus, X_test, y_train, y_test):\n",
    "    X_train = model.transform(train_corpus.T)\n",
    "\n",
    "    log_reg = LogisticRegressionCV(multi_class='multinomial')\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    pred_labels = log_reg.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, pred_labels, average='micro')\n",
    "\n",
    "\n",
    "def get_tm_metrics(model, train_corpus, test_corpus, dense_corpus, y_train, y_test):\n",
    "    W = model.get_topics().T\n",
    "    H = np.zeros((model.num_topics, len(test_corpus)))\n",
    "    for bow_id, bow in enumerate(test_corpus):\n",
    "        for topic_id, factor in model.get_document_topics(bow):\n",
    "            H[topic_id, bow_id] = factor\n",
    "\n",
    "    pred_factors = W.dot(H)\n",
    "    \n",
    "    l2_norm = get_tm_l2_norm(pred_factors, dense_corpus)\n",
    "    \n",
    "    pred_factors /= pred_factors.sum(axis=0)\n",
    "\n",
    "    perplexity = get_tm_perplexity(pred_factors, dense_corpus)\n",
    "\n",
    "    f1 = get_tm_f1(model, train_corpus, H.T, y_train, y_test)\n",
    "\n",
    "    model.normalize = True\n",
    "\n",
    "    coherence = CoherenceModel(\n",
    "        model=model,\n",
    "        corpus=test_corpus,\n",
    "        coherence='u_mass'\n",
    "    ).get_coherence()\n",
    "    \n",
    "    topics = model.show_topics(5)\n",
    "\n",
    "    model.normalize = False\n",
    "\n",
    "    return dict(\n",
    "        perplexity=perplexity,\n",
    "        coherence=coherence,\n",
    "        l2_norm=l2_norm,\n",
    "        f1=f1,\n",
    "        topics=topics,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tm_perplexity(pred_factors, dense_corpus):\n",
    "    return np.exp(-(np.log(pred_factors, where=pred_factors > 0) * dense_corpus).sum() / dense_corpus.sum())\n",
    "\n",
    "\n",
    "def get_tm_l2_norm(pred_factors, dense_corpus):\n",
    "    return np.linalg.norm(dense_corpus - pred_factors)\n",
    "\n",
    "\n",
    "def get_sklearn_topics(model, id2word, top_n=5):\n",
    "    topic_probas = model.components_.T\n",
    "    topic_probas = topic_probas / topic_probas.sum(axis=0)\n",
    "    \n",
    "    sparsity = np.zeros(topic_probas.shape[1])\n",
    "\n",
    "    for row in topic_probas:\n",
    "        sparsity += (row == 0)\n",
    "\n",
    "    sparsity /= topic_probas.shape[1]\n",
    "    \n",
    "    topic_probas = topic_probas[:, sparsity.argsort()][:, :top_n]\n",
    "    \n",
    "    token_indices = topic_probas.argsort(axis=0)[:10, :]\n",
    "    topic_probas.sort(axis=0)\n",
    "    topic_probas = topic_probas[:10, :]\n",
    "    \n",
    "    topics = []\n",
    "    \n",
    "    for topic_idx in range(topic_probas.shape[1]):\n",
    "        tokens = [\n",
    "            id2word[token_idx]\n",
    "            for token_idx\n",
    "            in token_indices[:, topic_idx]\n",
    "        ]\n",
    "        topic = (\n",
    "            '{}*\"{}\"'.format(round(proba, 3), token)\n",
    "            for proba, token\n",
    "            in zip(topic_probas[:, topic_idx], tokens)\n",
    "        )\n",
    "        topic = \" + \".join(topic)\n",
    "        topics.append((topic_idx, topic))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "\n",
    "def get_sklearn_metrics(model, train_corpus, test_corpus, y_train, y_test, dictionary):\n",
    "    W = model.components_.T\n",
    "    H = model.transform((test_corpus).T).T\n",
    "    pred_factors = W.dot(H)\n",
    "    \n",
    "    l2_norm = np.linalg.norm(test_corpus - pred_factors)\n",
    "    \n",
    "    pred_factors /= pred_factors.sum(axis=0)\n",
    "    \n",
    "    perplexity = np.exp(\n",
    "        -(np.log(pred_factors, where=pred_factors > 0) * test_corpus).sum()\n",
    "        / test_corpus.sum()\n",
    "    )\n",
    "\n",
    "    f1 = get_sklearn_f1(model, train_corpus, H.T, y_train, y_test)\n",
    "    \n",
    "    topics = get_sklearn_topics(model, dictionary, top_n=5)\n",
    "\n",
    "    return dict(\n",
    "        perplexity=perplexity,\n",
    "        l2_norm=l2_norm,\n",
    "        f1=f1,\n",
    "        topics=topics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tm_metrics = pd.DataFrame(columns=['model', 'train_time', 'perplexity', 'coherence', 'l2_norm', 'f1', 'topics'])\n",
    "\n",
    "train_dense_corpus = matutils.corpus2dense(train_corpus, len(dictionary))\n",
    "test_dense_corpus = matutils.corpus2dense(test_corpus, len(dictionary))\n",
    "\n",
    "trainset_target = [doc['target'] for doc in trainset]\n",
    "testset_target = [doc['target'] for doc in testset]\n",
    "\n",
    "# LDA metrics\n",
    "row = dict()\n",
    "row['model'] = 'lda'\n",
    "row['train_time'], lda = get_execution_time(\n",
    "    lambda: LdaModel(**fixed_params)\n",
    ")\n",
    "row.update(get_tm_metrics(\n",
    "    lda, train_corpus, test_corpus, test_dense_corpus, trainset_target, testset_target,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "\n",
    "# Sklearn NMF metrics\n",
    "row = dict()\n",
    "row['model'] = 'sklearn_nmf'\n",
    "sklearn_nmf = SklearnNmf(n_components=5, tol=1e-5, max_iter=int(1e9), random_state=42)\n",
    "row['train_time'], sklearn_nmf = get_execution_time(\n",
    "    lambda: sklearn_nmf.fit((train_dense_corpus).T)\n",
    ")\n",
    "row.update(get_sklearn_metrics(\n",
    "    sklearn_nmf, train_dense_corpus, test_dense_corpus, trainset_target, testset_target, dictionary\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "\n",
    "row = dict()\n",
    "row['model'] = 'gensim_nmf'\n",
    "row['train_time'], gensim_nmf = get_execution_time(\n",
    "    lambda: GensimNmf(\n",
    "        normalize=False,\n",
    "        **fixed_params\n",
    "    )\n",
    ")\n",
    "row.update(get_tm_metrics(\n",
    "    gensim_nmf, train_corpus, test_corpus, test_dense_corpus, trainset_target, testset_target,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "tm_metrics.replace(np.nan, '-', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_metrics.drop('topics', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main insights\n",
    "\n",
    "- Gensim NMF is **ridiculously** fast and leaves LDA and Sklearn far behind in terms of training time\n",
    "- Gensim NMF beats sklearn NMF implementation on f1 metric, though not on the l2 norm\n",
    "- Gensim NMF beats LDA on coherence, but LDA is still better on perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topics(tm_metrics):\n",
    "    for _, row in tm_metrics.iterrows():\n",
    "        print('\\n{}:'.format(row.model))\n",
    "        print(\"\\n\".join(str(topic) for topic in row.topics))\n",
    "        \n",
    "compare_topics(tm_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF's are on par with each other, LDA looks a bit worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faces Dataset Decomposition + Gensim NMF\n",
    "\n",
    "NMF algorithm works not only with texts, but with all kinds of stuff!\n",
    "\n",
    "Let's compare our model with the other factorization algorithms and check out the results!\n",
    "\n",
    "To do that we'll patch sklearn's [Faces Dataset Decomposition](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn wrapper\n",
    "Let's create a wrapper to compare Gensim NMF with the other factorizations on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "\n",
    "class NmfWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bow_matrix, **kwargs):\n",
    "        self.corpus = sparse.csc.csc_matrix(bow_matrix)\n",
    "        self.nmf = GensimNmf(**kwargs)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.nmf.update(self.corpus)\n",
    "\n",
    "    @property\n",
    "    def components_(self):\n",
    "        return self.nmf.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified FDD notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================\n",
    "Faces dataset decompositions\n",
    "============================\n",
    "\n",
    "This example applies to :ref:`olivetti_faces` different unsupervised\n",
    "matrix decomposition (dimension reduction) methods from the module\n",
    ":py:mod:`sklearn.decomposition` (see the documentation chapter\n",
    ":ref:`decompositions`) .\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Authors: Vlad Niculae, Alexandre Gramfort\n",
    "# License: BSD 3 claus\n",
    "\n",
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "# #############################################################################\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# List of the different estimators, whether to center and transpose the\n",
    "# problem, and whether the transformer uses the clustering API.\n",
    "estimators = [\n",
    "    ('Eigenfaces - PCA using randomized SVD',\n",
    "     decomposition.PCA(n_components=n_components, svd_solver='randomized',\n",
    "                       whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Non-negative components - NMF (Sklearn)',\n",
    "     decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),\n",
    "     False),\n",
    "\n",
    "    ('Non-negative components - NMF (Gensim)',\n",
    "     NmfWrapper(\n",
    "         bow_matrix=faces.T,\n",
    "         chunksize=3,\n",
    "         eval_every=400,\n",
    "         passes=2,\n",
    "         id2word={idx: idx for idx in range(faces.shape[1])},\n",
    "         num_topics=n_components,\n",
    "         minimum_probability=0,\n",
    "         random_state=42,\n",
    "     ),\n",
    "     False),\n",
    "\n",
    "    ('Independent components - FastICA',\n",
    "     decomposition.FastICA(n_components=n_components, whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Sparse comp. - MiniBatchSparsePCA',\n",
    "     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,\n",
    "                                      n_iter=100, batch_size=3,\n",
    "                                      random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('MiniBatchDictionaryLearning',\n",
    "     decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n",
    "                                               n_iter=50, batch_size=3,\n",
    "                                               random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Cluster centers - MiniBatchKMeans',\n",
    "     MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,\n",
    "                     max_iter=50, random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Factor Analysis components - FA',\n",
    "     decomposition.FactorAnalysis(n_components=n_components, max_iter=2),\n",
    "     True),\n",
    "]\n",
    "\n",
    "# #############################################################################\n",
    "# Plot a sample of the input data\n",
    "\n",
    "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])\n",
    "\n",
    "# #############################################################################\n",
    "# Do the estimation and plot it\n",
    "\n",
    "for name, estimator, center in estimators:\n",
    "    print(\"Extracting the top %d %s...\" % (n_components, name))\n",
    "    t0 = time.time()\n",
    "    data = faces\n",
    "    if center:\n",
    "        data = faces_centered\n",
    "    estimator.fit(data)\n",
    "    train_time = (time.time() - t0)\n",
    "    print(\"done in %0.3fs\" % train_time)\n",
    "    if hasattr(estimator, 'cluster_centers_'):\n",
    "        components_ = estimator.cluster_centers_\n",
    "    else:\n",
    "        components_ = estimator.components_\n",
    "\n",
    "    # Plot an image representing the pixelwise variance provided by the\n",
    "    # estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,\n",
    "    # via the PCA decomposition, also provides a scalar noise_variance_\n",
    "    # (the mean of pixelwise variance) that cannot be displayed as an image\n",
    "    # so we skip it.\n",
    "    if (hasattr(estimator, 'noise_variance_') and\n",
    "            estimator.noise_variance_.ndim > 0):  # Skip the Eigenfaces case\n",
    "        plot_gallery(\"Pixelwise variance\",\n",
    "                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n",
    "                     n_row=1)\n",
    "    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n",
    "                 components_[:n_components])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Gensim NMF implementation works as fast as Sklearn NMF and achieves comparable quality, even though it's not optimised for dense matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Gensim NMF is an extremely fast and memory-optimized model, and should be used whenever your system resources are too scarse for the task or when you want to try something different from LDA."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.1",
    "jupytext_version": "0.8.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
