{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between sklearn's and gensim's implementations of NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "from gensim.models.nmf import Nmf as GensimNmf\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim import matutils\n",
    "from sklearn.decomposition.nmf import NMF as SklearnNmf\n",
    "import sklearn.decomposition.nmf\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "2018-06-05 13:03:04,253 : INFO : Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "2018-06-05 13:03:04,255 : INFO : Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "documents = preprocess_documents(fetch_20newsgroups().data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-05 13:03:26,753 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-06-05 13:03:26,886 : INFO : built Dictionary(17622 unique tokens: ['addit', 'bodi', 'bricklin', 'brought', 'bumper']...) from 1000 documents (total 136081 corpus positions)\n",
      "2018-06-05 13:03:26,906 : INFO : discarding 14411 tokens: [('bricklin', 2), ('bumper', 4), ('edu', 661), ('funki', 4), ('lerxst', 2), ('line', 989), ('organ', 952), ('rac', 1), ('subject', 1000), ('tellm', 2)]...\n",
      "2018-06-05 13:03:26,907 : INFO : keeping 3211 tokens which were in no less than 5 and no more than 500 (=50.0%) documents\n",
      "2018-06-05 13:03:26,912 : INFO : resulting dictionary: Dictionary(3211 unique tokens: ['addit', 'bodi', 'brought', 'call', 'car']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "dictionary.filter_extremes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    dictionary.doc2bow(document)\n",
    "    for document\n",
    "    in documents\n",
    "]\n",
    "\n",
    "bow_matrix = matutils.corpus2dense(corpus, len(dictionary), len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 875 ms, sys: 385 ms, total: 1.26 s\n",
      "Wall time: 711 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%prun\n",
    "\n",
    "sklearn_nmf = SklearnNmf(n_components=5, tol=1e-5, max_iter=int(1e9), random_state=42)\n",
    "\n",
    "W = sklearn_nmf.fit_transform(bow_matrix)\n",
    "H = sklearn_nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f sklearn.decomposition.nmf._fit_coordinate_descent sklearn_nmf.fit_transform(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482.0895496423899"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(bow_matrix - W.dot(H), 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-05 13:08:47,801 : INFO : Loss (no outliers): 593.2289895825425\tLoss (with outliers): 593.2289895825425\n",
      "2018-06-05 13:08:48,289 : INFO : Loss (no outliers): 502.92121729493823\tLoss (with outliers): 502.92121729493823\n",
      "2018-06-05 13:08:48,805 : INFO : Loss (no outliers): 486.88611940507246\tLoss (with outliers): 486.88611940507246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 s, sys: 1.82 s, total: 4.29 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%prun\n",
    "\n",
    "PASSES = 3\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "gensim_nmf = GensimNmf(\n",
    "    corpus,\n",
    "    chunksize=len(corpus),\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    lambda_=1000,\n",
    "    kappa=1.,\n",
    "    passes=PASSES,\n",
    "    normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f GensimNmf._solve_w GensimNmf(corpus, chunksize=len(corpus), num_topics=5, id2word=dictionary, lambda_=1., kappa=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gensim_nmf.get_topics().T\n",
    "H = np.hstack(gensim_nmf[bow] for bow in corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484.3090246375028"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(matutils.corpus2dense(corpus, len(dictionary), len(documents)) - W.dot(H), 'fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.117*\"jesu\" + 0.065*\"matthew\" + 0.035*\"peopl\" + 0.033*\"christian\" + 0.031*\"prophet\" + 0.029*\"dai\" + 0.028*\"said\" + 0.027*\"messiah\" + 0.024*\"come\" + 0.023*\"king\"'),\n",
       " (1,\n",
       "  '0.049*\"armenian\" + 0.030*\"peopl\" + 0.018*\"turkish\" + 0.016*\"post\" + 0.015*\"russian\" + 0.014*\"genocid\" + 0.013*\"time\" + 0.013*\"year\" + 0.012*\"com\" + 0.012*\"articl\"'),\n",
       " (2,\n",
       "  '0.359*\"max\" + 0.007*\"umd\" + 0.005*\"hst\" + 0.002*\"gee\" + 0.001*\"distribut\" + 0.001*\"univers\" + 0.001*\"repli\" + 0.001*\"usa\" + 0.001*\"keyword\" + 0.001*\"net\"'),\n",
       " (3,\n",
       "  '0.083*\"health\" + 0.060*\"us\" + 0.041*\"year\" + 0.038*\"report\" + 0.035*\"state\" + 0.033*\"diseas\" + 0.032*\"case\" + 0.031*\"public\" + 0.030*\"ag\" + 0.030*\"person\"'),\n",
       " (4,\n",
       "  '0.105*\"argument\" + 0.064*\"conclus\" + 0.056*\"exampl\" + 0.052*\"premis\" + 0.051*\"true\" + 0.034*\"occur\" + 0.032*\"logic\" + 0.031*\"fals\" + 0.029*\"form\" + 0.028*\"assert\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_nmf.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the personal experience I can say that the higher number of passes and shuffle of the trainset significantly improves performance.\n",
    "\n",
    "Then, of course, you should perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image of stars\n",
    "### (For the sake of visualization of performance on sparse trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-19dd13884f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stars_scaled.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('stars_scaled.jpg').convert('L')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_matrix = np.uint8(img.getdata()).reshape(img.size[::-1])\n",
    "img_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sklearn_nmf = SklearnNmf(n_components=10, tol=1e-5, max_iter=int(1e9))\n",
    "\n",
    "W = sklearn_nmf.fit_transform(img_matrix)\n",
    "H = sklearn_nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(W.dot(H)), 'L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "img_corpus = matutils.Dense2Corpus(img_matrix[np.random.choice(img_matrix.shape[0], img_matrix.shape[0], replace=False)].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "\n",
    "gensim_nmf = GensimNmf(\n",
    "    img_corpus,\n",
    "    chunksize=len(corpus),\n",
    "    num_topics=10,\n",
    "    passes=2,\n",
    "    id2word={k: k for k in range(img_matrix.shape[1])},\n",
    "    lambda_=1000,\n",
    "    kappa=1,\n",
    "    normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gensim_nmf.get_topics().T\n",
    "H = np.hstack(gensim_nmf[bow] for bow in matutils.Dense2Corpus(img_matrix.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructed matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(W.dot(H).T), 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
